import numpy as np
import torch
import h5py

from lightning import Callback, LightningModule, Trainer
from collections import defaultdict
from pathlib import Path


class TestEvalWriter(Callback):
    def __init__(
            self,
            write_inputs: bool,
            write_outputs: bool,
            write_preds: bool,
            write_targets: bool,
            write_losses: bool,
            write_layers: list[str] = ["final"],
            ):
        super().__init__()

        self.write_inputs = write_inputs
        self.write_outputs = write_outputs
        self.write_preds = write_preds
        self.write_targets = write_targets
        self.write_losses = write_losses
        self.write_layers = write_layers

    def setup(self, trainer: Trainer, module: LightningModule, stage: str) -> None:
        super().setup(trainer=trainer, pl_module=module, stage=stage)
        self.trainer = trainer
        self.module = module
        self.dataset = trainer.datamodule.test_dataloader().dataset

    @property
    def output_path(self) -> Path:
        # The output dataset will be saved in the same direcory as the checkpoint
        out_dir = Path(self.trainer.ckpt_path).parent
        out_basename = str(Path(self.trainer.ckpt_path).stem)
        return Path(out_dir / f"{out_basename}_testeval.h5")

    def on_test_start(self, trainer: Trainer, module: LightningModule) -> None:
        # Open the handle for writing to the file
        self.file = h5py.File(self.output_path, "w")

    def on_test_batch_end(self, trainer, module, batch, batch_idx, dataloader_idx):        
        inputs, outputs, targets, preds, losses = batch

        # TODO: Standardise this somehow for dataloders that have a
        # batched input / do not have event names
        event_name = self.dataset.event_names[batch_idx]
        sample_group = self.file.create_group(event_name)

        # Write inputs and targets
        if self.write_inputs:
            self.write_items(sample_group, "inputs", inputs)

        if self.write_targets:
            self.write_items(sample_group, "targets", targets)

        # Items produced by model have layer/task structure
        if self.write_outputs:
            self.write_layer_task_items(sample_group, "outputs", outputs)
        
        if self.write_preds:
            self.write_layer_task_items(sample_group, "preds", preds)
        
        if self.write_losses:
            self.write_layer_task_items(sample_group, "losses", losses) 

    def write_items(self, sample_group, item_name, items):
        # This will write out a dict of items that has the structure
        # sample/item/layer/task/value, e.g.
        # event_name/inputs/pixel_x
        items_group = sample_group.create_group(item_name)
        for name, value in items.items():
            self.create_dataset(items_group, name, value)

    def write_layer_task_items(self, sample_group, item_name, items):
        items_group = sample_group.create_group(item_name)
        # This will write out a dict of items that has the structure
        # sample/item/layer/task/value, e.g.
        # event_name/preds/final/track_regression/track_phi
        for layer_name, layer_items in items.items():
            # Only write items fow the specified layers
            if layer_name not in self.write_layers:
                continue
            layer_group = items_group.create_group(layer_name)
            for task_name, task_items in layer_items.items():
                task_group = layer_group.create_group(task_name)
                for name, value in task_items.items():
                    self.create_dataset(task_group, name, value)

    def on_test_epoch_end(self, trainer, module):
        # Close the file handle now we are done
        self.file.close()
        print("Created output file", self.output_path)

    def create_dataset(self, group, value, name):
        # Shouldn't need to detach as we are testing
        if isinstance(value, torch.Tensor):
            value = value.cpu().numpy()

        # Handle half precison values
        value_type = np.dtype(value.dtype)
        if value_type.kind == "f" and value_type.element_size() == 2:
            value = value.astype(np.float16)

        # Write the data to the file
        group.create_dataset(name, data=value, compression="lzf")


